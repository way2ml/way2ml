---
pageClass: ml-class
---

<!--
 * @Description: 
 * @Author: Jack Huang
 * @Github: https://github.com/HuangJiaLian
 * @Date: 2019-10-25 20:35:39
 * @LastEditors: Jack Huang
 * @LastEditTime: 2019-10-28 16:12:59
 -->
 
# WGAN

## 原始的GAN有什么缺陷?
GAN的目标函数:
$$
L\left(D, g_{\theta}\right)=\mathbb{E}_{x \sim \mathbb{P}_{r}}[\log D(x)]+\mathbb{E}_{x \sim \mathbb{P}_{g}}[\log (1-D(x))]
$$

判别器想要 $L$ 越大越好, 生成器想要这个值越小越好。

判别器越好，生成器梯度消失越严重。

为什么, 两方面去解释:

当判别器最优时, 即目标函数对D的一阶导数为零时得到:

$$
D^*(x) = \frac{P_r(x)}{P_r(x)+P_g(x)}
$$

此时固定住$D^*$,将其带入到目标函数,结合KL和JS可以得到:

$$
L\left(D^{*}, g_{\theta}\right)=2 J S D\left(\mathbb{P}_{r} \| \mathbb{P}_{g}\right)-2 \log 2
$$

只要不是$P_g(x)$和$P_r(x)$不重叠，或者说重叠部分可以忽略，那么$JSD(P_r||P_g)＝c$。

::: warning 疑问
其实这里为什么是一个常数我还不是很清楚。
:::

固定D后要做的事是优化G,此时我们的目标是想要$L\left(D^{*}, g_{\theta}\right)$的值越小越好，此时我们会发现，这个值是一个常数，也就是说G调整参数$\theta$后,
使得$P_g$变成$P_{g'}$, 却发现$L$没有发生任何变化，于是便放弃调整。所以便G便永远也学不会。

::: tip 思考
这其实反映两一个哲理。表扬一个孩子的进步才是最重要的，不要说孩子聪明。
:::

也就是说使用现在的目标函数得到 $D^*$ 后想要训练得到 $G^*$ 是基本没有希望的。

::: tip 你可能会问
- 那为什么一般的RL用一个给定的Reward却可以学起来?
因为一般情况给的Reward都不是最优的那个Reward

- 那我们不给让D学得那么好不久可以了吗?
说的有道理，但这里的度是什么，谁能说得清楚呢? 如果不够严格,G是不会进步的，如果太严格也是不会进步的。
:::

## 缺陷背后的数学
当$P_r$与$P_g$的支撑集（support）是高维空间中的低维流形（manifold）时，$P_r$与$P_g$重叠部分测度（measure）为0的概率为1。

- 支撑集（support） 其实就是函数的非零部分子集，比如[ReLU函数](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0)的支撑集就是$(0, +\infty)$，一个概率分布的支撑集就是所有概率密度非零部分的集合。

- 流形（manifold） 是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。

- 测度（measure） 是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。

::: tip 思考
这让我想到了一个机器学习的分支,流形学习。
:::

## 如何解决这个问题? 
现在的问题就是JS距离不是一个很好的距离, 因此我们想要说换一个距离。这个距离可以很好地区分出:$P_g$到$P_{g'}$的改变是好还是不好。


参考:

[知乎:求简要介绍一下流形学习的基本思想？](https://www.zhihu.com/question/24015486)
[知乎:令人拍案叫绝的Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913)