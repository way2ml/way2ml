(window.webpackJsonp=window.webpackJsonp||[]).push([[105],{313:function(t,n,r){"use strict";r.r(n);var e=r(0),o=Object(e.a)({},(function(){var t=this,n=t.$createElement,r=t._self._c||n;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"asynchronous-advantage-actor-critic"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#asynchronous-advantage-actor-critic"}},[t._v("#")]),t._v(" Asynchronous Advantage Actor-Critic")]),t._v(" "),r("p",[t._v("Asynchronous Advantage Actor-Critic 怎么发音？\n[eɪˈsɪŋkrənəs]")]),t._v(" "),r("p",{attrs:{align:"center"}},[r("audio",{ref:"A3C",attrs:{src:"/images/ml/RL/A3C/A3C.mp3",controls:"",loop:"",preload:""}})]),t._v(" "),r("p",[t._v("上次在PPO那一节中的Advantage Function怎么设计还没有说完，这一节来继续探讨这个问题。")]),t._v(" "),r("p",[t._v("Actor-Critics就是通过V得到Advantage Function, 然后做Policy Gridient, 得到好的策略。而得到V function的方式是使用MC或者TD。")]),t._v(" "),r("p",[t._v("Reinforce是Policy Gradient的其中一种方法，你在不知不觉中已经学了它了。")]),t._v(" "),r("p",[t._v("关于为什么要在Actor的Loss里面添加Entropy的项，这里有一个"),r("a",{attrs:{href:"https://github.com/dennybritz/reinforcement-learning/issues/34",target:"_blank",rel:"noopener noreferrer"}},[t._v("讨论"),r("OutboundLink")],1),t._v("。")]),t._v(" "),r("p",[t._v("网络上唯一一个按照这里提到的方法写的[程序]。(https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20Actor%20Critic%20Solution.ipynb)")]),t._v(" "),r("Livere")],1)}),[],!1,null,null,null);n.default=o.exports}}]);