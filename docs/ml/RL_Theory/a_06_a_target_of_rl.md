---
pageClass: ml-class
---
# 强化学习的目标
强化学习究竟是学习什么? 这肯定也是你的问题。强化学习的目的是为了找到一套较好的策略。即让智能体学会做一系列的动作使得期望回报最大。例如在游戏中强化学习的目标就是**Learn to maximize the expected cumulative reward per episode**,即学习一套动作使得每一个episode中的累计奖励的期望最大。例如在Space Invaders游戏中就是学习在游戏结束之前尽可能多的杀死外星人，同时学会避让子弹，让自己活的更久。这里又涉及到一个问题什么样子的策略算是好的策略呢? 没有办法，我们不得不给出一个好策略的定义。给出下面一个定义:
- 说有一个策略$\pi$比另外一个策略$\pi'$好或者一样， 要求在$\pi$策略下每一个状态$s$期望的回报(return)都比$\pi'$策略下的回报大或者一样。因为状态期望的回报$\mathbb{E}_{\pi}[G_t|S_t=s]$就是值函数$v_{\pi}(s)$的定义。因此换句话说，仅当$v_{\pi}(s) \geq v_{\pi'}(s)$对所有的$s\in \mathcal{S}$都成立时，我们就说$\pi \geq \pi'$。

我们总是可以找到至少一个策略$\pi$比其他策略好或者与之相等。那么这里的这个策略$\pi$就叫做**最优策略(optimal policy)**。虽然可能有多个最优策略，把它们表示成$\pi_{*}$。最优策略可能有多个，但是最优策略们却都对应相同的值函数，我们把这个值函数叫做**最优状态值函数 optimal state-value function**, 把它表示成$v_{*}$。对所有的$s \in \mathcal{S}$都有:
$$
v_{*}(s) \doteq \max \limits_{\pi}\{v_{\pi}(s)\}
$$

最优策略们同时也对应了相同的**最优动作值函数 optimal action-value**, 把它表示成$q_{*}$。对应所有的$s,a \in \mathcal{S},\mathcal{A}$都有:
$$
q_{*}(s,a) \doteq \max \limits_{\pi} \{q_{\pi}(s,a)\}
$$

==强化学习的目标就是找到策略$\pi_{*}$，使得它可以使得我们的长期回报最大化。==

<Livere/>