(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{267:function(t,a,r){"use strict";r.r(a);var e=r(0),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,r=t._self._c||a;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"一些关于gan的思考"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#一些关于gan的思考"}},[t._v("#")]),t._v(" 一些关于GAN的思考")]),t._v(" "),r("h2",{attrs:{id:"gan大体的结构是什么样子"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gan大体的结构是什么样子"}},[t._v("#")]),t._v(" GAN大体的结构是什么样子?")]),t._v(" "),r("p",[t._v("生成对抗神经网络有两部分:")]),t._v(" "),r("ul",[r("li",[r("strong",[t._v("生成器")]),t._v("学习产生可信的(plausible)的数据。生成的数据立即变成负样本去训练判别器。")]),t._v(" "),r("li",[r("strong",[t._v("判别器")]),t._v("学习分辨生成器的假数据和真实的数据。判别器惩罚生成器产生的不真实的数据。")])]),t._v(" "),r("h2",{attrs:{id:"判别器"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#判别器"}},[t._v("#")]),t._v(" 判别器")]),t._v(" "),r("p",[t._v("判别器就仅仅是一个二分类器。")]),t._v(" "),r("p",[t._v("训练数据:")]),t._v(" "),r("ul",[r("li",[t._v("真实数据，例如真人的照片，作为训练判别器的正样本")]),t._v(" "),r("li",[t._v("假数据(生成器生成的数据), 作为训练判别器的负样本。")])]),t._v(" "),r("p",[t._v("训练过程:")]),t._v(" "),r("ul",[r("li",[r("ol",[r("li",[t._v("判别器去分辨真实数据和假数据")])])]),t._v(" "),r("li",[r("ol",{attrs:{start:"2"}},[r("li",[t._v("判别器的loss会惩罚错误的判断(将真的判别成假的或者将假的判别成真的);也就是说错得越多loss越大。")])])]),t._v(" "),r("li",[r("ol",{attrs:{start:"3"}},[r("li",[t._v("根据loss通过backpropagation更新判断器的参数")])])])]),t._v(" "),r("h2",{attrs:{id:"gan最终达到的效果是什么"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gan最终达到的效果是什么"}},[t._v("#")]),t._v(" GAN最终达到的效果是什么?")]),t._v(" "),r("p",[r("strong",[t._v("生成器")]),t._v("训练得足够好。 判别器变得足够差，它开始将假的数据判别成真的, 判断的准确率开始下降。")]),t._v(" "),r("div",{staticClass:"custom-block warning"},[r("p",{staticClass:"custom-block-title"},[t._v("上面的意思是说判别器到最后没有一点用吗?")]),t._v(" "),r("p",[t._v("不是判别器一点用都没有，而是判别器的输出对于优化生成器没有用了。我个人认为判别器只是不能够判断出很好的")])]),t._v(" "),r("h2",{attrs:{id:"discriminator不改变可以训练好generator吗"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#discriminator不改变可以训练好generator吗"}},[t._v("#")]),t._v(" Discriminator不改变可以训练好Generator吗?")]),t._v(" "),r("div",{staticClass:"custom-block warning"},[r("p",{staticClass:"custom-block-title"},[t._v("可以拿训练好的Discriminator再从头训练得到一个Generator吗?")])]),t._v(" "),r("h2",{attrs:{id:"gan中的两个模型谁比较难训练"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gan中的两个模型谁比较难训练"}},[t._v("#")]),t._v(" GAN中的两个模型谁比较难训练?")]),t._v(" "),r("p",[t._v("生成模型要模型化一个非常复杂的分布")]),t._v(" "),r("h2",{attrs:{id:"gan很难判断是否收敛是什么意思"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#gan很难判断是否收敛是什么意思"}},[t._v("#")]),t._v(" GAN很难判断是否收敛是什么意思?")]),t._v(" "),r("p",[t._v("随着生成器性能在训练过程中的不断提升,判别器的性能变得越来越差, 因为它不能很容易地分辨出真假。\n如果生成器在这个博弈的过程中完全取胜,那么判别器的准确率就变成了50%。\n也就是说这个时候的判别器在扔硬币在做判断。")]),t._v(" "),r("p",[t._v("也就是说，随着时间的变化，判别器的反馈(输出)变得越来越没有意义。\n如果判别器都在抛硬币做判断了,你还在继续训练，也就是说这个时候生成器得到的反馈是没有任何意义的。\n那么你的生成器的性能就可能会下降。")]),t._v(" "),r("div",{staticClass:"custom-block tip"},[r("p",{staticClass:"custom-block-title"},[t._v("思考")]),t._v(" "),r("p",[t._v("这里我们说的性能越来越差的意思是判别器很难判别真的和很好的,并不是不能判断好的差的；\n这里我们说的判别器的反馈(输出)变得越来越没有意义，并不是说判别器本身并没有意义，而是对于优化G的参数并没有意义。")]),t._v(" "),r("p",[t._v("我认为这一点非常重要:\n我认为李宏毅老师说的最后的D会烂掉的意思绝对不是D本身没有意义，而是说D最后对于优化G没有任何意义。")])]),t._v(" "),r("p",[t._v("因此对于GAN来说,收敛转瞬即逝的, 而不是到达一个稳态。")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://developers.google.com/machine-learning/gan/training",target:"_blank",rel:"noopener noreferrer"}},[t._v("原文"),r("OutboundLink")],1)]),t._v(" "),r("h2",{attrs:{id:"用一句话描述gan"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#用一句话描述gan"}},[t._v("#")]),t._v(" 用一句话描述GAN")]),t._v(" "),r("p",[t._v("GAN是在重复一个概率分布。")]),t._v(" "),r("h2",{attrs:{id:"loss-function"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#loss-function"}},[t._v("#")]),t._v(" Loss Function")]),t._v(" "),r("p",[t._v("既然是重复一个概率分布我们就需要一个Loss Function来反应真实数据的概率分布和生成数据的概率分布之间的距离。")]),t._v(" "),r("h2",{attrs:{id:"实际中gan会遇到的问题"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#实际中gan会遇到的问题"}},[t._v("#")]),t._v(" 实际中GAN会遇到的问题")]),t._v(" "),r("p",[t._v("GAN有很多失败的情况。所有的这些都是Active Reserch这个领域的问题, 然而这些问题到现在一个都还没有解决。")]),t._v(" "),r("ol",[r("li",[t._v("Vanishing Gradients")])]),t._v(" "),r("p",[r("a",{attrs:{href:"https://arxiv.org/pdf/1701.04862.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Research"),r("OutboundLink")],1),t._v(" has suggested\nthat if your discriminator is too good, then generator training can fail due to\n"),r("a",{attrs:{href:"https://en.wikipedia.org/wiki/Vanishing_gradient_problem",target:"_blank",rel:"noopener noreferrer"}},[t._v("vanishing gradients"),r("OutboundLink")],1),t._v(".\nIn effect, an optimal discriminator doesn't provide enough information\nfor the generator to make progress.")]),t._v(" "),r("ol",{attrs:{start:"2"}},[r("li",[r("p",[t._v("Mode Collapse")])]),t._v(" "),r("li",[r("p",[t._v("Failure to Converge")])])]),t._v(" "),r("h2",{attrs:{id:"原始的gan的问题是什么"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#原始的gan的问题是什么"}},[t._v("#")]),t._v(" 原始的GAN的问题是什么?")]),t._v(" "),r("p",[t._v("一句话概括：判别器越好，生成器梯度消失越严重。")]),t._v(" "),r("p",[t._v("参考:")]),t._v(" "),r("p",[r("a",{attrs:{href:"https://developers.google.com/machine-learning/gan",target:"_blank",rel:"noopener noreferrer"}},[t._v("谷歌GAN教程"),r("OutboundLink")],1),r("br"),t._v(" "),r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/25071913",target:"_blank",rel:"noopener noreferrer"}},[t._v("知乎:令人拍案叫绝的Wasserstein GAN"),r("OutboundLink")],1),r("br"),t._v(" "),r("a",{attrs:{href:"https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b",target:"_blank",rel:"noopener noreferrer"}},[t._v("Why it is so hard to train Generative Adversarial Networks!"),r("OutboundLink")],1),r("br"),t._v(" "),r("a",{attrs:{href:"https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("From GAN to WGAN"),r("OutboundLink")],1),r("br")])])}),[],!1,null,null,null);a.default=n.exports}}]);