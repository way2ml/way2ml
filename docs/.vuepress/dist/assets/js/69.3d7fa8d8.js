(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{276:function(e,t,a){"use strict";a.r(t);var n=a(0),o=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"为什么ppo不能够解决moutain-car的问题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#为什么ppo不能够解决moutain-car的问题"}},[e._v("#")]),e._v(" 为什么PPO不能够解决Moutain_Car的问题?")]),e._v(" "),a("p",[e._v("Sparse rewards. In OpenAI Gym MountainCar you only get a positive reward when you reach the top.")]),e._v(" "),a("p",[e._v("PPO is an on-policy algorithm. It performs a policy gradient update after each episode and throws the\ndata away. Reaching the goal in MountainCar by random actions is a pretty rare event. When it finally happens, it's very unlikely that a single policy gradient update will be enough to start reaching the goal consistently, so PPO gets stuck again with no learning signal until it reaches the goal again by chance.")]),e._v(" "),a("p",[e._v("On the other hand, DDPG stores this event in the replay buffer so it does not forget.\nThe TD bootstrapping of the Q function will eventually propagate the reward from the goal backwards into the Q estimate for other states near the goal.")]),e._v(" "),a("p",[e._v("This is a big advantage of off-policy RL algorithms.")]),e._v(" "),a("p",[e._v("Also DDPG uses an Ornstein-Uhlenbeck process for time-correlated exploration,\nwhereas PPO samples Gaussian noise. The Ornstein-Uhlenbeck process is more likely to generate useful exploratory actions.\n(The exploration methods are not immutable properties of the algorithms, just the Baselines implementations.)")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.reddit.com/r/reinforcementlearning/comments/9o8ez0/ppo_struggling_at_mountaincar_whereas_ddpg_is/",target:"_blank",rel:"noopener noreferrer"}},[e._v("参考"),a("OutboundLink")],1)])])}),[],!1,null,null,null);t.default=o.exports}}]);